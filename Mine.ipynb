{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_40291_2.1\"></a>\n",
    "## Gradient descent summary\n",
    "So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In lecture, *gradient descent* was described as:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, parameters $w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b): \n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters    \n",
    "      b (scalar):  model parameter     \n",
    "      \n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(w,x) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194082\n"
     ]
    }
   ],
   "source": [
    "# get a row from our training data\n",
    "x_vec = X_train[0,:]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec,w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_4\"></a>\n",
    "# 4 Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$ \n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "      cost = cost+(np.dot(w,X[i])+b - y[i])**2\n",
    "    cost = (1/(2*m))*cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904330213735e-12\n"
     ]
    }
   ],
   "source": [
    "# Compute and display cost using our pre-chosen optimal parameters. \n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"toc_15456_5\"></a>\n",
    "# 5 Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    # tinh shape\n",
    "    m,n = X.shape\n",
    "    # tinh gia tri dao ham cua w_j\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_dw1 = np.zeros((n,))\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "      dif=np.dot(w,X[i])+b - y[i]\n",
    "      dj_db = dj_db + dif\n",
    "      dj_dw = dj_dw + dif*X[i]\n",
    "    dj_dw = dj_dw/m\n",
    "    dj_db = dj_db/m\n",
    "    return dj_db,dj_dw\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: -1.6739251122999121e-06\n",
      "dj_dw at initial w,b: \n",
      " [-2.72623577e-03 -6.27197263e-06 -2.21745571e-06 -6.92403379e-05]\n"
     ]
    }
   ],
   "source": [
    "#Compute and display gradient \n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**:   \n",
    "dj_db at initial w,b: -1.6739251122999121e-06  \n",
    "dj_dw at initial w,b:   \n",
    " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters  \n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters \n",
    "      b (scalar)       : Updated value of parameter \n",
    "      \"\"\"\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "    J_hist = []\n",
    "    for i in range(num_iters):\n",
    "        dj_db,dj_dw=gradient_function(X, y, w, b)\n",
    "        w = w - alpha*dj_dw\n",
    "        b = b - alpha*dj_db\n",
    "    \n",
    "        J_hist.append(cost_function(X, y, w, b))\n",
    "    return w,b,J_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b,w found by gradient descent: -0.00,[ 0.20396569  0.00374919 -0.0112487  -0.0658614 ] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent \n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient, \n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result**:    \n",
    "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07]   \n",
    "prediction: 426.19, target value: 460  \n",
    "prediction: 286.17, target value: 232  \n",
    "prediction: 171.47, target value: 178  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(J_hist)-990),J_hist[:10],marker=\"x\")\n",
    "# plt.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
